{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e3c280-881a-4c7b-8134-f265171a4db1",
   "metadata": {},
   "source": [
    "# Tools and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7963a-b2ab-4c4f-b0e4-a4d32278a5ed",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a826d01-0e4e-41ee-a6af-3b0aade870f5",
   "metadata": {},
   "source": [
    "### 1. Tools :\n",
    "\n",
    "* Website to choose Tools : https://python.langchain.com/v0.1/docs/integrations/tools/\n",
    "\n",
    "* Tools in langchain helps us integrate the Gen AI app with multiple external resources or tools like extenal databases, API's, Web scraping and even custom tools etc....\n",
    "\n",
    "* You can call these tools during the conversation to get real-time data or perform actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0316c6-c3b6-40d6-984d-23459371d091",
   "metadata": {},
   "source": [
    "### 2. Agents\n",
    "\n",
    "* Agents are decision-makers that use tools to complete a task.\n",
    "\n",
    "* They can choose which tool to use, when to use it, and how to interact with it based on the input they receive.\n",
    "\n",
    "* So, this helps to manage task effectively by making choices based on context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030518c-bf4c-4d82-86f7-59560ab56a71",
   "metadata": {},
   "source": [
    "### Example \n",
    "\n",
    "##### Imagine you're building a chatbot that answers questions about weather:\n",
    "\n",
    "* Tool: An API that gives real-time weather data.\n",
    "\n",
    "* Agent: The agent decides when to call the weather API based on the user's question and then sends the result back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db35f47-95db-44de-b607-db3e7c682689",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68fdeb-155e-49eb-9c82-d401262235f4",
   "metadata": {},
   "source": [
    "# Search Engine With Tools and Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ed90a8-a40e-4935-bcc0-355a48919eb8",
   "metadata": {},
   "source": [
    "##### Pre-Requiste\n",
    "\n",
    "* Install \"arxiv\"\n",
    "* Install \"wikipedia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28232c13-f122-4997-a2f6-2aac8d7438b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Import the .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import openai\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv(\"HF_TOKEN\") \n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da1115-bfee-421b-8186-0d94ca5b7397",
   "metadata": {},
   "source": [
    "#### Creating Tool (wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b9e54b-75d4-4c68-82d1-8ac59cc32ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper, ArxivAPIWrapper # these wrappers are needed to run ArxivQueryRun, WikipediaQueryRun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02550fb-cbc4-486f-88d2-45f11ff410d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used the inbuilt tool of wikipedia\n",
    "api_wrapper_wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max = 250) # top k represents the number of output, chars max represnt how much charcter used to respond.\n",
    "wiki = WikipediaQueryRun(api_wrapper = api_wrapper_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a207bc-a7e3-425d-8fe1-ec8e13800cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used the inbuilt tool of arxiv\n",
    "api_wrapper_arxiv = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max = 250)\n",
    "arxiv = ArxivQueryRun(api_wrapper = api_wrapper_arxiv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fd0d4c-dab5-4119-b1c5-26581d84bca1",
   "metadata": {},
   "source": [
    "##### Create Custom tools [RAG Tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "422c7aa3-3c80-4715-914d-a1fbd419777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2ab6ed3-3d88-4079-9c7e-01268bcd6ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eedf005ca19346d4af221b07b23f52cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jemim\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\jemim\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8923627eaba9443aa044f62c33ab8b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d877c4502204fbb9a35f066009fe656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb52e7e6d5141f9ae727d5522aecf31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4240023ce65e44eca39303e7837e99fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8168052ca43a46b5a92a389a61a1ce85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0a07453bec48468dc848c6cd30b1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036b2f6acae04ed6a8e45d525c37ff94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc407da2864e40e180cb5b92c8e1d439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d9bbd3ee41469985affb27865f43c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee36f4bb6ea44a98c362fbfdeec9d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)\n",
    "documents = documents.split_documents(docs)\n",
    "\n",
    "vectordb = FAISS.from_documents(documents, HuggingFaceEmbeddings())\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c29694-5503-4aa2-bbec-5244a865798b",
   "metadata": {},
   "source": [
    "#### Convert the retriever into tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a441f1a-d4ef-4288-b15e-7b872ad13612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(retriever, \"langsmith-search\",\"Search any info about langsmith\")\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906091b3-8c29-418b-91e5-8a61a29e4f68",
   "metadata": {},
   "source": [
    "##### Combine all 3 tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f8f44b2-2ead-4125-91f7-c1d5f2325bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wiki,arxiv,retriever_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766ead0-8bae-49b8-aacc-c47a6673da1b",
   "metadata": {},
   "source": [
    "##### Run all these tools with Agents and LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "347a117e-c0e1-47ce-9338-0774cf2c1144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000016E2221A770>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000016E2221BB20>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"Llama3-8b-8192\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a19b7ef9-ebbf-46a1-bdaa-fae4abe92f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way of prompt template\n",
    "\n",
    "from langchain import hub\n",
    "prompt = hub.pull('hwchase17/openai-functions-agent')\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1febb-3267-4004-ae72-68ed3ed2d0ba",
   "metadata": {},
   "source": [
    "##### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cd9b8fb-1d15-4810-91d7-53f6f3efa80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_tools_agent\n",
    "agent = create_openai_tools_agent(llm,tools,prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c091f179-6e29-43ee-9ada-144728239c10",
   "metadata": {},
   "source": [
    "##### Execute Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cddee74f-b8ca-4c8c-8a1e-eb3ae2164ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000016E3084CB80>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000016E3084CB80>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000016E2221A770>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000016E2221BB20>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search any info about langsmith', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'C:\\\\Users\\\\jemim\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)), Tool(name='langsmith-search', description='Search any info about langsmith', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000016E30AAEE60>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016E0EFB0CA0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000016E30DEB7F0>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016E0EFB0CA0>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent = agent, tools = tools, verbose=True)\n",
    "agent_executor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bb789c4-d1c8-4901-b98d-b2c1b4ab3d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith-search` with `{'query': 'Langsmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mGet started with LangSmith | ðŸ¦œï¸ðŸ› ï¸ LangSmith\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith is framework-agnostic â€”Â it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observabilityâ€‹\n",
      "Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\n",
      "This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmithâ€™s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\n",
      "\n",
      "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic â€” score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "\n",
      "Get started by adding tracing to your application.\n",
      "Create dashboards to view key metrics like RPS, error rates and costs.\n",
      "\n",
      "Evalsâ€‹\n",
      "The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\n",
      "\n",
      "Get started by creating your first evaluation.\n",
      "Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\n",
      "Analyze results of evaluations in the LangSmith UI and compare results over time.\n",
      "Easily collect human feedback on your data to improve your application.\n",
      "\n",
      "Prompt Engineeringâ€‹\n",
      "\u001b[32;1m\u001b[1;3mIt seems that LangSmith is a platform for building production-grade Large Language Model (LLM) applications. It provides features such as observability, evaluation, and prompt engineering to help developers closely monitor and evaluate their applications, iterate on prompts, and ship quickly with confidence.\n",
      "\n",
      "Some of the key features of LangSmith include:\n",
      "\n",
      "* Observability: LangSmith allows developers to analyze traces, configure metrics, and set up dashboards and alerts to monitor their application's performance.\n",
      "* Evaluation: LangSmith provides a set of tools to build and run high-quality evaluations, allowing developers to assess the performance of their application and collect human feedback to improve it.\n",
      "* Prompt Engineering: LangSmith provides a set of tools to enable and facilitate prompt engineering, allowing developers to find the perfect prompt for their application.\n",
      "\n",
      "It also seems that LangSmith is closely integrated with LangChain, an open-source framework for building AI applications, and LangGraph, a platform for building and deploying LLM applications.\n",
      "\n",
      "Overall, LangSmith appears to be a comprehensive platform for building and deploying LLM applications, providing a range of features to help developers ship high-quality applications quickly and efficiently.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Tell me about Langsmith',\n",
       " 'output': \"It seems that LangSmith is a platform for building production-grade Large Language Model (LLM) applications. It provides features such as observability, evaluation, and prompt engineering to help developers closely monitor and evaluate their applications, iterate on prompts, and ship quickly with confidence.\\n\\nSome of the key features of LangSmith include:\\n\\n* Observability: LangSmith allows developers to analyze traces, configure metrics, and set up dashboards and alerts to monitor their application's performance.\\n* Evaluation: LangSmith provides a set of tools to build and run high-quality evaluations, allowing developers to assess the performance of their application and collect human feedback to improve it.\\n* Prompt Engineering: LangSmith provides a set of tools to enable and facilitate prompt engineering, allowing developers to find the perfect prompt for their application.\\n\\nIt also seems that LangSmith is closely integrated with LangChain, an open-source framework for building AI applications, and LangGraph, a platform for building and deploying LLM applications.\\n\\nOverall, LangSmith appears to be a comprehensive platform for building and deploying LLM applications, providing a range of features to help developers ship high-quality applications quickly and efficiently.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"Tell me about Langsmith\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bdbb29b-e7cd-4f0a-89b3-6672bc40b023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Machine learning in 2 lines'}`\n",
      "\n",
      "\n",
      "\u001b[36;1m\u001b[1;3mPage: List of datasets for machine-learning research\n",
      "\u001b[32;1m\u001b[1;3me datasets are used in machine learning (ML) research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major adva\u001b[0m\n",
      "Invoking: `wikipedia` with `{'query': 'machine learning'}`\n",
      "\n",
      "\n",
      "\u001b[36;1m\u001b[1;3mPage: Machine learning\n",
      "\u001b[32;1m\u001b[1;3mHere's a 2-line summary based on the results:ficial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks wit\u001b[0m\n",
      "\n",
      "Machine learning is a field of artificial intelligence that enables algorithms to learn from data and make predictions or decisions without being explicitly programmed. It involves developing statistical models that can generalize to new, unseen data and perform tasks such as classification, regression, clustering, and more.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is machine learning in 2 lines',\n",
       " 'output': \"Here's a 2-line summary based on the results:\\n\\nMachine learning is a field of artificial intelligence that enables algorithms to learn from data and make predictions or decisions without being explicitly programmed. It involves developing statistical models that can generalize to new, unseen data and perform tasks such as classification, regression, clustering, and more.\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"what is machine learning in 2 lines\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d275736-4465-4071-93f9-2458ded35dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mno\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'are you using my Chat History to answer my question.. say yes or no',\n",
       " 'output': 'no'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\":\"are you using my Chat History to answer my question.. say yes or no\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e586953-2ad7-4c57-836d-a7a4c0904cc1",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24e85c3-275d-49b6-8729-d22b77318739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
